\section{Research Experience on Optimization Theory}
\begin{itemize}  
    \item \heading{Convergence and Speedup Analysis of Distributed Optimization Algorithms}{Peking University}
    {Advisor: Kun Yuan, Peking University}{Nov.\ 2023 -- Jun.\ 2025}
          \begin{itemize}
            %   \item \detail{Advisor}{\href{https://kunyuan827.github.io}{Kun Yuan}}{}
              \item \textbf{Push-Pull Algorithm Provably Achieves Linear
Speedup Over Arbitrary Network Topologies}\\
\href{https://lavaei.ieor.berkeley.edu/Group.html}{Liyuan Liang}*, \textbf{\underline{Gan Luo}*},  \href{https://kunyuan827.github.io}{Kun Yuan} \href{https://arxiv.org/abs/2506.18075}{[Arxiv]} \href{https://github.com/pkumelon/PushPull}{[Code: Linear Speedup]} \href{https://luoxiaogan.github.io/GanLuo.github.io/PDFs/new_proof.pdf}{[Notes]}
              \begin{itemize}
%               \item \textbf{Push-Pull Algorithm Provably Achieves Linear
% Speedup Over Arbitrary Network Topologies}\\
% \href{https://lavaei.ieor.berkeley.edu/Group.html}{Liyuan Liang}*, \underline{Gan Luo}*,  \href{https://kunyuan827.github.io}{Kun Yuan} (*Equal contribution) \href{https://arxiv.org/abs/2506.18075}{[Arxiv]}
                  \item {\textbf{Submitted to SIAM Journal on Optimization.}}
                %   \item {Conducted research on the \href{https://arxiv.org/pdf/1810.06653v4}{Push-Pull Algorithm}, focusing on convergence and linear speedup properties in non-convex and stochastic settings on arbitrary topology.}
                %   \item We propose a novel multi-step descent analysis framework and first to prove that the \href{https://arxiv.org/pdf/1810.06653v4}{Push-Pull Algorithm} achieves linear speedup over arbitrary strongly connected digraphs.
                %   \item This is my first research project, started in December 2023, advised by Prof. Kun Yuan and in collaboration with Liyuan Liang, whom I am fortunate to learn from. At first, we wanted to use the[traditional approach] to analyze the problem, but after several months we found that this would give a non-vanishing noise term in the upper bound. We further found that this was because we first analyzed the single-term noise and then added them together, which motivated us to try a multi-step descent analysis framework, and finally we succeeded at about October 2024. You can see how we clearly analyze the multi-step term in the [notes].
                %   \item Validated the proposed theoretical results by conducting distributed optimization numerical experiments on the MNIST and CIFAR10 datasets.
                \item Proposed a novel multi-step descent analysis framework and first proved that the \href{https://arxiv.org/pdf/1810.06653v4}{Push-Pull algorithm} achieves linear speedup over arbitrary strongly connected digraphs. Our multi-step analysis resolved the non-vanishing noise issue inherent in \href{https://arxiv.org/abs/2312.04928}{traditional single-step approaches}. Also see the \href{https://luoxiaogan.github.io/GanLuo.github.io/PDFs/new_proof.pdf}{[notes]}.

                \item Conducted all numerical experiments to validate the linear speedup property we proved.   
              \end{itemize}
              
              \item \textbf{Achieving Linear Speedup and Optimal Complexity for Decentralized Optimization over Row-stochastic Networks}\\
              \href{https://lavaei.ieor.berkeley.edu/Group.html}{Liyuan Liang}*, \href{https://openreview.net/profile?id=~Xinyi_Chen9}{Xinyi Chen}*, \textbf{\underline{Gan Luo}*}, \href{https://kunyuan827.github.io}{Kun Yuan} \href{https://arxiv.org/pdf/2506.04600}{[Arxiv]} \href{https://github.com/Luoxiaogan/ICML2025_project}{[Code]}
              \begin{itemize}
            %   \item \textbf{Achieving Linear Speedup and Optimal Complexity for Decentralized Optimization over Row-stochastic Networks}\\
            %   \href{https://lavaei.ieor.berkeley.edu/Group.html}{Liyuan Liang}*, \href{https://openreview.net/profile?id=~Xinyi_Chen9}{Xinyi Chen}*, \underline{Gan Luo}*, \href{https://kunyuan827.github.io}{Kun Yuan} (*Equal contribution) \href{https://arxiv.org/pdf/2506.04600}{[Arxiv]}
                  \item {\textbf{Acceped to ICML 2025, Spotlight}}
                %   \item {Introduced effective metrics to capture the influence of row-stochastic mixing matrices}
                %   \item {Established the first convergence lower bound for decentralized learning over row-stochastic networks}
                %   \item {Incorporated a multi-step gossip (MG) protocol, to attain the lower bound, achieving optimal complexity.}
                %   \item {Proposed a novel analysis framework demonstrating that \href{https://arxiv.org/pdf/1803.09169}{PULL-DIAG-GT} achieves linear speedup, which is the first such result for row-stochastic decentralized optimization.}
                %   \item{Conducted numerical experiments to validate theoretical results.}
                \item {Introduced novel metrics to characterize the influence of row-stochastic mixing matrices and established the first convergence lower bound for decentralized optimization over row-stochastic networks.}
                \item {Developed a new analysis framework proving that \href{https://arxiv.org/pdf/1803.09169}{PULL-DIAG} achieves linear speedup and proposed a multi-gossip protocol that resolves instability issues and attains the lower bound with near-optimal complexity.}
                \item {Conducted all numerical experiments to validate the theoretical results on convergence lower bound, linear speedup, and near-optimal complexity.}
              \end{itemize}
          \end{itemize}
          
          
\end{itemize}