\section{Research Experience on Large Language Models and Operations Research}
\begin{itemize}
    \item \heading{Online Scheduling on LLM Inference}{Massachusetts Institute of Technology}
          {Advisor: Prof. David Simchi-Levi, Massachusetts Institute of Technology}{Oct.\ 2024 -- May.\ 2025 }
          \begin{itemize}
            %   \item \detail{Advisor}{\href{https://slevi1.mit.edu}{David Simchi-Levi}}{}
              \item \textbf{Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory Constraints}\\
              ($\alpha$-$\beta$) \href{https://www.mit.edu/~aorc/index.html}{Ruicheng Ao}*, \textbf{\underline{Gan Luo}*}, \href{https://slevi1.mit.edu}{David Simchi-Levi}, \href{https://www.columbia.edu/~xw2230/}{Xinshang Wang} \href{https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5195463}{[SSRN]} \href{https://arxiv.org/abs/2504.11320}{[Arxiv]} \href{https://github.com/Luoxiaogan/vidur_or}{[Code1]} \href{https://github.com/Luoxiaogan/vllm_simulation}{[Code2]} \href{https://or4llm.github.io}{[Demo]}
              \begin{itemize}
                  \item {\textbf{Submitted to Operations Research.}}
                  \item {\textbf{Preliminary version accepted to NeurIPS 2025 MLxOR Workshop.}}
                  \item {\textbf{Finalist in INFORMS Applied Probability Society Best Student Paper Prize, 2025}. \href{https://connect.informs.org/aps/apsawards/awards}{[Link]} \href{https://luoxiaogan.github.io/GanLuo.github.io/images/best_paper_finalist_pdf_20251119.pdf}{[Award]}}
                  \item {Formulated the LLM inference as a multi-stage online scheduling task with stochastic queueable requests, proposed a novel online batching algorithm for LLM inference and proved that the algorithm achieves near-optimal throughput while controlling latency and Time to First Token (TTFT). }
                  \item {Conducted numerical experiments on synthetic and real-world datasets with Llama-7B on A100 GPU to validate theoretical results.}
              \end{itemize}
          \end{itemize}

    \item \heading{Analysis of batching and scheduling algorithms in LLM inference}{Columbia University}
          {Advisor: Prof. Jing Dong, Columbia University}{Apr.\ 2025 -- Present }
          \begin{itemize}
              \item \textbf{Work in progress, Analysis of Continuous Batching Algorithm in LLM Inference}
              \begin{itemize}
    %             \item \textbf{Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory Constraints}\\
    % ($\alpha$-$\beta$) \href{https://www.mit.edu/~aorc/index.html}{Ruicheng Ao}*, \underline{Gan Luo}*, \href{https://slevi1.mit.edu}{David Simchi-Levi}, \href{https://www.columbia.edu/~xw2230/}{Xinshang Wang}, available on \href{https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5195463}{[SSRN]} and \href{https://arxiv.org/abs/2504.11320}{[Arxiv]}
                  \item {In this work, we first formulated the continuos batching algorithm as a discrete-time model. Then we analyzed its steady state and its dynamics behavior under overloaded conditions. Next we will futher analyze its dynamics under admission control.}
              \end{itemize}
          \end{itemize}
    
    \item \heading{LLM Agent and Workflow Generation}{DAMO Academy}
          {Advisor: Prof. Wotao Yin, DAMO Academy \& Prof. Bin Dong, Peking University}{Apr.\ 2025 -- Present }
          \begin{itemize}
              \item \textbf{MetaFlow: A Meta Approach of Training LLMs into Generalizable Workflow Generators}\\
              ($\alpha$-$\beta$) \textbf{\underline{Gan Luo}*}, \href{https://openreview.net/profile?id=~Zihan_Qin5}{Zihan Qin}*, \href{http://faculty.bicmr.pku.edu.cn/~dongbin/}{Bin Dong}, \href{https://wotaoyin.mathopt.com}{Wotao Yin}
              \begin{itemize}
                  \item {\textbf{Submitted, Under Review.}}
                  \item {Formulated workflow generation as a meta-learning problem where LLMs learn to compose task-level solution strategies from operators, producing reusable workflows that generalize across problem instances rather than instance-specific solutions.}
              \item {Developed a two-stage training approach combining supervised fine-tuning on synthetic workflow data with reinforcement learning with verifiable rewards (RLVR), using execution feedback across instances to improve end-to-end success rates.}
              \item {Demonstrated strong zero-shot generalization to untrained tasks and novel operator sets, achieving performance comparable to state-of-the-art baselines on in-domain tasks across benchmarks in question answering, code generation, and mathematical reasoning.}
              \end{itemize}
          \end{itemize}
\end{itemize}